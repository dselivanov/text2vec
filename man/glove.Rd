% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/model_GloVe.R
\name{GloVe}
\alias{GloVe}
\alias{glove}
\title{Creates GloVe word-embeddings model.}
\usage{
GloVe(word_vectors_size, vocabulary, x_max, learning_rate = 0.15,
  max_cost = 10, alpha = 0.75, lambda = 0, shuffle_seed = NA_integer_,
  grain_size = 100000L, ...)

glove(tcm, word_vectors_size, x_max, num_iters, shuffle_seed = NA_integer_,
  learning_rate = 0.15, verbose = TRUE, convergence_tol = -1,
  grain_size = 100000L, max_cost = 10, alpha = 0.75, lambda = 0,
  dump_every_n = 0L, ...)
}
\arguments{
\item{word_vectors_size}{desired dimenson for word vectors}

\item{vocabulary}{\code{character} vector or instanceof
\code{text2vec_vocabulary} class. Each word should correspond to dimension
of cooccurence matrix.}

\item{x_max}{maximum number of co-occurrences to use in the weighting
function. See the GloVe paper for details:
\url{http://nlp.stanford.edu/pubs/glove.pdf}.}

\item{learning_rate}{learning rate for SGD. I do not recommend that you
modify this parameter, since AdaGrad will quickly adjust it to optimal.}

\item{max_cost}{the maximum absolute value of calculated gradient for any
single co-occurrence pair. Try to set this to a smaller value if you have
problems with numerical stability.}

\item{alpha}{the alpha in weighting function formula : \eqn{f(x) = 1 if x >
x_max; else (x/x_max)^alpha}}

\item{lambda}{\code{numeric}, L1 regularization coefficient.
\code{0} by default, as in original paper and implementation. Sometimes it
worth to try to set it to small number.
Something like \code{lambda = 1e-5} usually works fine.}

\item{shuffle_seed}{\code{integer} seed. Use \code{NA_integer_} to turn
shuffling off. A seed defines shuffling before each SGD iteration.
Generelly shuffling is a good idea for stochastic-gradient descent, but
from my experience in this particular case it does not improve convergence.
By default there is no shuffling. Please report if you find that shuffling
improves your score.}

\item{grain_size}{I do not recommend adjusting this paramenter. This is the
grain_size for \code{RcppParallel::parallelReduce}. For details, see
\url{http://rcppcore.github.io/RcppParallel/#grain-size}.}

\item{...}{arguments passed to other methods (not used at the moment).}

\item{tcm}{an object which represents a term-co-occurrence matrix, which is
used in training. Preferably \code{dgTMatrix}.}

\item{num_iters}{number of iterations}

\item{verbose}{\code{logical} whether to display training inforamtion}

\item{convergence_tol}{defines early stopping strategy. We stop fitting
when one of two following conditions will be satisfied: (a) we have used
all iterations, or (b) \code{cost_previous_iter / cost_current_iter - 1 <
convergence_tol}.}

\item{dump_every_n}{\code{integer} - dump word vectors each \code{dump_every_n} epoch
to \code{word_vec_history} attribute.}
}
\description{
\bold{Iterative algorithm}. This function creates a GloVe word-embeddings model.
It can be trained via fully can asynchronous and parallel
AdaGrad with \code{fit_predict} function.
}
\examples{
\dontrun{
temp <- tempfile()
download.file('http://mattmahoney.net/dc/text8.zip', temp)
text8 <- readLines(unz(temp, "text8"))
it <- itoken(text8, preprocess_function = identity,
             tokenizer = function(x) strsplit(x, " ", TRUE))
vocab <- create_vocabulary(it) \%>\%
 prune_vocabulary(term_count_min = 5)
it <- itoken(text8)
tcm <- create_tcm(it, vocab_vectorizer(vocab, grow_dtm = FALSE, skip_grams_window = 5L))

glove_model <- GloVe(word_vectors_size = 50, vocabulary = vocab,
 x_max = 10, learning_rate = 0.25)
# fit model and get word vectors
wv = fit_predict(glove_model, tcm, n_iter = 10)
# glove_model is mutable object! (actually it is a closure)
wv2 = predict(glove_model)
identical(wv, wv2)
}
}
\seealso{
\url{http://nlp.stanford.edu/projects/glove/}
}

